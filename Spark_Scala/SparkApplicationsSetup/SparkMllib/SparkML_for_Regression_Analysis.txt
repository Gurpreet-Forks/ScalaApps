To understand 'Equation of straight line' and 'regression analysis'

https://youtu.be/BekcPu_3Bqw
https://youtu.be/zPG4NjIkCjc
https://youtu.be/JvS2triCgOY

Regression analysis is a predictive modelling technique.
It estimates the relationship between a dependent (target) and an independent variable(predictor)
Note** x-variable can increase its value as much as it wants .So, for an arbitaray value of x , we are
predicting value of y. Prediction is done using regression.

Linear Regression:
Problem: 
To find Yearly Amount spent (dependent variable) based on info such as "Avg Session Length","Time on App","Time on Website",
"Length of Membership" [independent variables]


Sample Dataset: EcommerceCustomers.csv

Solution:

//import relevant packages
import org.apache.spark.ml.evaluation.RegressionEvaluator
import org.apache.spark.ml.regression.LinearRegression
import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}
import org.apache.log4j._
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.mllib.linalg.Vectors

Logger.getLogger("org"(.setLevel(Level.ERROR)

val data = spark.read.option("header","true").option("inferSchema","true").format("csv").load("EcommerceCustomers.csv")

val ecommDF = data
ecommDF.printSchema()

//Filtering columns
val ecommDF1 = ecommDF.select(ecommDF("Yearly Amount Spent").as("label"),$"Avg Session Length",$"Time on App",
$"Time on Website",$"Length of Membership")

//ML expects data in a N-dimensional vector format i.e. Matrix format
//refer "https://spark.apache.org/docs/latest/ml-features.html"
//refer "https://spark.apache.org/docs/2.1.0/ml-features.html#vectorassembler"
//Thus we can use a feature transformer called 'VectorAssembler'

val assembler = new VectorAssembler().setInputCols(Array("Avg Session Length","Time on App","Time on Website",
"Length of Membership")).setOutputCol("features")

val ecommDF2 = assembler.transform(ecommDF1).select($"label",$"features")

ecommDF2.show()

//creating an object of LinearRegression
val lr = new LinearRegression()

val lrModel = lr.fit(ecommDF2)

//extracting information such as coefficients,intercepts etc..
println(s"Coefficients: ${lrModel.coefficients} Intercept: ${lrModel.intercept}")

val trainingSummary = lrModel.summary

println(s"numIterations: ${trainingSummary.totalIterations}")

println(s"objectiveHistory: ${trainingSummary.objectiveHistory.toList}")

trainingSummary.residuals.show()

===================================================
Logistic Regression:

Technique for traditional statistics and machine learning mainly to solve classification problems.
Here outcome variable(result) is binary(1/0 , true/false, yes/no) based on predictor variables .

Its same as linear regression but instead it predicts whether something is true or false,instead of predicting something
continous.
Instead of fitting into a straight like, it fits points into "s" shaped logistic function.{sigmoid function}
[based on threshold value set, we decide the output from the function]
Logistic regression is usally used for classification
It can work with continous data or discrete data.
Here we test if a variable's effect on the prediction is significantly different than 0.
If not, then, variable is not helping the prediction.Logistic regression's ability to provide
probabilities and classify new samples using continous and discrete measurements makes it a 
popular machine learning method.
With linear regression we fit the line for data points using "least square" method.
ie we find the line that minimizes the sum of squares of these residuals.
LR doesn't have concept of residuals.Instead it uses something called as "maximum likelihood"
where in through iterations the curve with the maximum likelihood is selected.

Problem: To predict if a customer would be bankrupt/or not and if would be able to pay back bank's loan.

Sample Dataset:Qualitative_Bankruptcy.txt

Solution:

import org.apache.spark.mlib.evaluation.MulticlassMetrics
import org.apache.spark.mlib.classification.{LogisticRegressionWithLBFGS, LogisticRegressionModel}
import org.apache.spark.mlib.regression.LabeledPoint
import org.apache.spark.mlib.linalg.{Vector, Vectors}

//schema of data file Attribute Information: {P=Positive,A=Average,N=Negative,B-Bankruptcy,NB=Non-Bankruptcy}

1. Industrial Risk: {P,A,N}
2. Management Risk: {P,A,N}
3. Financial Flexibility: {P,A,N}
4. Credibility: {P,A,N}
5. Competitveness: {P,A,N}
6. Operating Risk: {P,A,N}
7. Class: {B,NB}

//Transform each qualitative data in the dataset into a double numeric value 

def getDoubleValue( input: String) : Double = {
    var result:Double = 0.0
    if ((input == "P") result =  3.0
    if ((input == "A") result =  2.0
    if ((input == "N") result =  1.0
    if ((input == "NB") result = 1.0
    if ((input == "B") result =  0.0
    return result
    }

//Read data into memory-in lazy loading
val data = sc.textFile("Qualitative_Bankruptcy.txt")
data.count() 

//Prepare data for the logistic regression algorithm
val parsedData = data.map{line => 
val parts = line.split(",")
LabeledPoint(getDoubleValue(parts(6)),Vectors.dense(parts.slice(0,6).map(x => getDoubleValue(x))))
}

println(parsedData.take(10).mkString("\n"))

//split data into training(60%) and test (40%)
val splits = parsedData.randomSplit(Array(0.6,0.4),seed = 11L)
val trainingData = splits(0)
val testData = splits(1)

//train the model
val model = new LogisticRegressionWithLBFGS().setNumClasses(2).run(trainingData)

//evaluate model on training examples and compute training error
val labelAndPreds = testData.map { point =>
val prediction = model.predict(point.features)
(point.label, prediction)
}

val trainErr = labelAndPreds.filter(r => r._1 != r._2).count.toDouble /testData.count
println("Training Error = " + trainErr)
========================


